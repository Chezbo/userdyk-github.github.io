---
layout : post
title : PL03-Topic02, Keras
categories: [PL03-Topic02]
comments : true
tags : [PL03-Topic02]
---
[Back to the previous page](https://userdyk-github.github.io/pl03/PL03-Libraries.html) <br>
List of posts to read before reading this article
- <a href='https://userdyk-github.github.io/'>post1</a>
- <a href='https://userdyk-github.github.io/'>post2</a>
- <a href='https://userdyk-github.github.io/'>post3</a>

---

## Contents
{:.no_toc}

* ToC
{:toc}

<hr class="division1">

## **An Introduction to Deep Learning and Keras**

### ***A Sneak Peek into the Keras Framework***

```python
# Import required packages 
from keras.models import Sequential
from keras.layers import Dense
import numpy as np

# Getting the data ready 
# Generate train dummy data for 1000 Students and dummy test for 500
# Columns :Age, Hours of Study &Avg Previous test scores 
np.random.seed(2018)  #Setting seed for reproducibility 
train_data, test_data = np.random.random((1000, 3)), np.random.random((500, 3))

# Generate dummy results for 1000 students : Whether Passed (1) or Failed (0) 
train_labels = np.random.randint(2, size=(1000, 1))
test_labels = np.random.randint(2, size=(500, 1))

# Defining the model structure with the required layers, 
# of neurons, activation function and optimizers
model = Sequential() 
model.add(Dense(5, input_dim=3, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(train_data, train_labels, batch_size=32, epochs=3)
```
<details markdown="1">
<summary class='jb-small' style="color:blue">TRAINING RESULT</summary>
<hr class='division3'>
```
Epoch 1/3
1000/1000 [==============================] - 1s 1ms/step - loss: 0.6939 - acc: 0.4870
Epoch 2/3
1000/1000 [==============================] - 0s 41us/step - loss: 0.6934 - acc: 0.4830
Epoch 3/3
1000/1000 [==============================] - 0s 39us/step - loss: 0.6933 - acc: 0.4880
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Testing the Model Performance</summary>
<hr class='division3'>
```python
results = model.evaluate(test_data, test_labels)
for i in range(len(model.metrics_names)):
    print(model.metrics_names[i]," : ", results[i])
```
`OUTPUT`
```
500/500 [==============================] - 0s 813us/step
loss  :  0.692422465801239
acc  :  0.5200000002384185
```
<hr class='division3'>
</details>


<br><br><br>
<hr class="division2">

## **Keras in Action**

|step 1|Defining the Model Structure|
|step 2|Training the Model|
|step 3|Making Predictions|

### ***Putting All the Building Blocks Together***

```python
# Import required packages 
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.datasets import boston_housing

#Download the data using Keras; this will need an active internet connection
(x_train, y_train), (x_test, y_test) = boston_housing.load_data()


#Extract the last 100 rows from the training data to create the validation datasets. 
x_val = x_train[300:,]       # last 300 row
y_val = y_train[300:,]       # last 300 row

#Define the model architecture
model = Sequential()
model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))
model.add(Dense(6, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal'))

# Compile model
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])

#Train the model
model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_val,y_val))
```
<details markdown="1">
<summary class='jb-small' style="color:blue">TRAINING RESULT</summary>
<hr class='division3'>
```
Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz
57344/57026 [==============================] - 0s 2us/step

Train on 404 samples, validate on 104 samples
Epoch 1/3
404/404 [==============================] - 1s 2ms/step - loss: 580.9334 - mean_absolute_percentage_error: 99.4362 - val_loss: 662.7088 - val_mean_absolute_percentage_error: 98.0074
Epoch 2/3
404/404 [==============================] - 0s 51us/step - loss: 554.0448 - mean_absolute_percentage_error: 95.8691 - val_loss: 618.2133 - val_mean_absolute_percentage_error: 93.0662
Epoch 3/3
404/404 [==============================] - 0s 50us/step - loss: 492.5111 - mean_absolute_percentage_error: 87.4437 - val_loss: 520.5322 - val_mean_absolute_percentage_error: 81.4210
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Testing the Model Performance</summary>
<hr class='division3'>
```python
results = model.evaluate(x_test, y_test)
for i in range(len(model.metrics_names)):
    print(model.metrics_names[i]," : ", results[i])
```
`OUTPUT`
```
102/102 [==============================] - 0s 62us/step
loss  :  463.09825942095586
mean_absolute_percentage_error  :  78.82570214365043
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">RETRAINING(after tuning fitting parameters)</summary>
<hr class='division3'>
```python
#Train the model 
model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_val,y_val))
```
`OUTPUT`
```
Train on 404 samples, validate on 104 samples
Epoch 1/30
404/404 [==============================] - 0s 58us/step - loss: 379.4086 - mean_absolute_percentage_error: 70.2558 - val_loss: 360.9293 - val_mean_absolute_percentage_error: 59.3759
Epoch 2/30
404/404 [==============================] - 0s 47us/step - loss: 228.6982 - mean_absolute_percentage_error: 48.1161 - val_loss: 198.9693 - val_mean_absolute_percentage_error: 40.2195
Epoch 3/30
404/404 [==============================] - 0s 49us/step - loss: 137.9041 - mean_absolute_percentage_error: 43.9592 - val_loss: 143.6277 - val_mean_absolute_percentage_error: 42.3353
Epoch 4/30
404/404 [==============================] - 0s 50us/step - loss: 128.0068 - mean_absolute_percentage_error: 48.4147 - val_loss: 136.8944 - val_mean_absolute_percentage_error: 40.8082
Epoch 5/30
404/404 [==============================] - 0s 55us/step - loss: 118.3841 - mean_absolute_percentage_error: 44.0519 - val_loss: 133.1419 - val_mean_absolute_percentage_error: 37.5794
Epoch 6/30
404/404 [==============================] - 0s 49us/step - loss: 111.5623 - mean_absolute_percentage_error: 40.6460 - val_loss: 128.8092 - val_mean_absolute_percentage_error: 35.8778
Epoch 7/30
404/404 [==============================] - 0s 50us/step - loss: 105.6107 - mean_absolute_percentage_error: 39.3712 - val_loss: 121.8723 - val_mean_absolute_percentage_error: 35.2191
Epoch 8/30
404/404 [==============================] - 0s 60us/step - loss: 100.5365 - mean_absolute_percentage_error: 38.5076 - val_loss: 116.7530 - val_mean_absolute_percentage_error: 34.0602
Epoch 9/30
404/404 [==============================] - 0s 55us/step - loss: 95.6473 - mean_absolute_percentage_error: 36.4750 - val_loss: 112.9952 - val_mean_absolute_percentage_error: 32.6358
Epoch 10/30
404/404 [==============================] - 0s 56us/step - loss: 91.3528 - mean_absolute_percentage_error: 35.8936 - val_loss: 107.6109 - val_mean_absolute_percentage_error: 32.4268
Epoch 11/30
404/404 [==============================] - 0s 62us/step - loss: 86.8670 - mean_absolute_percentage_error: 34.5687 - val_loss: 105.0734 - val_mean_absolute_percentage_error: 30.8377
Epoch 12/30
404/404 [==============================] - 0s 57us/step - loss: 83.8678 - mean_absolute_percentage_error: 33.1729 - val_loss: 100.9976 - val_mean_absolute_percentage_error: 30.1347
Epoch 13/30
404/404 [==============================] - 0s 79us/step - loss: 80.3338 - mean_absolute_percentage_error: 32.9414 - val_loss: 98.0269 - val_mean_absolute_percentage_error: 29.1839
Epoch 14/30
404/404 [==============================] - 0s 49us/step - loss: 77.0266 - mean_absolute_percentage_error: 31.3633 - val_loss: 96.0304 - val_mean_absolute_percentage_error: 28.0592
Epoch 15/30
404/404 [==============================] - 0s 69us/step - loss: 74.5713 - mean_absolute_percentage_error: 30.6367 - val_loss: 93.4559 - val_mean_absolute_percentage_error: 27.4387
Epoch 16/30
404/404 [==============================] - 0s 76us/step - loss: 72.4605 - mean_absolute_percentage_error: 30.5300 - val_loss: 90.2561 - val_mean_absolute_percentage_error: 27.5172
Epoch 17/30
404/404 [==============================] - 0s 56us/step - loss: 70.2668 - mean_absolute_percentage_error: 30.4674 - val_loss: 89.6360 - val_mean_absolute_percentage_error: 26.2286
Epoch 18/30
404/404 [==============================] - 0s 59us/step - loss: 69.0524 - mean_absolute_percentage_error: 28.2962 - val_loss: 89.3691 - val_mean_absolute_percentage_error: 25.0894
Epoch 19/30
404/404 [==============================] - 0s 59us/step - loss: 67.1439 - mean_absolute_percentage_error: 28.7645 - val_loss: 86.4003 - val_mean_absolute_percentage_error: 25.7927
Epoch 20/30
404/404 [==============================] - 0s 65us/step - loss: 66.1058 - mean_absolute_percentage_error: 29.1919 - val_loss: 85.8313 - val_mean_absolute_percentage_error: 25.0127
Epoch 21/30
404/404 [==============================] - 0s 55us/step - loss: 65.2683 - mean_absolute_percentage_error: 27.9101 - val_loss: 85.7227 - val_mean_absolute_percentage_error: 24.2154
Epoch 22/30
404/404 [==============================] - 0s 57us/step - loss: 64.3430 - mean_absolute_percentage_error: 28.2435 - val_loss: 84.4628 - val_mean_absolute_percentage_error: 24.4009
Epoch 23/30
404/404 [==============================] - 0s 56us/step - loss: 63.5753 - mean_absolute_percentage_error: 27.9445 - val_loss: 83.9498 - val_mean_absolute_percentage_error: 24.2201
Epoch 24/30
404/404 [==============================] - 0s 62us/step - loss: 62.9955 - mean_absolute_percentage_error: 28.0562 - val_loss: 83.1876 - val_mean_absolute_percentage_error: 24.3564
Epoch 25/30
404/404 [==============================] - 0s 52us/step - loss: 62.5542 - mean_absolute_percentage_error: 27.7791 - val_loss: 83.8650 - val_mean_absolute_percentage_error: 23.6271
Epoch 26/30
404/404 [==============================] - 0s 53us/step - loss: 62.1328 - mean_absolute_percentage_error: 27.1743 - val_loss: 82.4939 - val_mean_absolute_percentage_error: 24.0875
Epoch 27/30
404/404 [==============================] - 0s 58us/step - loss: 61.6133 - mean_absolute_percentage_error: 27.6719 - val_loss: 82.2328 - val_mean_absolute_percentage_error: 23.8878
Epoch 28/30
404/404 [==============================] - 0s 61us/step - loss: 60.8626 - mean_absolute_percentage_error: 26.9424 - val_loss: 82.7886 - val_mean_absolute_percentage_error: 23.2897
Epoch 29/30
404/404 [==============================] - 0s 54us/step - loss: 60.6314 - mean_absolute_percentage_error: 26.1994 - val_loss: 81.6793 - val_mean_absolute_percentage_error: 23.6167
Epoch 30/30
404/404 [==============================] - 0s 52us/step - loss: 60.5237 - mean_absolute_percentage_error: 27.4995 - val_loss: 81.5046 - val_mean_absolute_percentage_error: 23.4413
<keras.callbacks.History at 0x7fd199e50c18>
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Re-Testing the Model Performance</summary>
<hr class='division3'>
```python
results = model.evaluate(x_test, y_test)
for i in range(len(model.metrics_names)):
    print(model.metrics_names[i]," : ", results[i])
```
`OUTPUT`
```
102/102 [==============================] - 0s 92us/step
loss  :  65.75437209185432
mean_absolute_percentage_error  :  31.099634806315105
```
<hr class='division3'>
</details>
<br><br><br>

<hr class="division2">

## **Deep Neural Networks for Supervised Learning: Regression**
<a href="https://www.kaggle.com/c/rossmann-store-sales/data" target="_blank">Dataset Download</a>
<br><br><br>

### ***Exploring the Data***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Data Engineering***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Defining Model Baseline Performance***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Designing the DNN***

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense, Dropout



df = pd.read_csv("train.csv")
store = pd.read_csv("store.csv")
df_new = df.merge(store, on=["Store"], how="inner")

# We can extract all date properties from a datetime datatype
df_new['Date'] = pd.to_datetime(df_new['Date'], infer_datetime_format=True)
df_new["Month"] = df_new["Date"].dt.month
df_new["Quarter"] = df_new["Date"].dt.quarter
df_new["Year"] = df_new["Date"].dt.year
df_new["Day"] = df_new["Date"].dt.day
df_new["Week"] = df_new["Date"].dt.week
df_new["Season"] = np.where(df_new["Month"].isin([3, 4, 5]), "Spring",
                            np.where(df_new["Month"].isin([6, 7, 8]), "Summer",
                                     np.where(df_new["Month"].isin([9, 10, 11]), "Fall",
                                              np.where(df_new["Month"].isin([12, 1, 2]), "Winter", "None"))))

# Replace nulls with the mode
df_new["CompetitionDistance"] = df_new["CompetitionDistance"].fillna(df_new["CompetitionDistance"].mode()[0])

# Double check if we still see nulls for the column
df_new["CompetitionDistance"].isnull().sum() / df_new.shape[0] * 100

# Define a variable for each type of feature
target = ["Sales"]
numeric_columns = ["Customers", "Open", "Promo", "Promo2", "StateHoliday", "SchoolHoliday", "CompetitionDistance"]
categorical_columns = ["DayOfWeek", "Quarter", "Month", "Year", "StoreType", "Assortment", "Season"]



# Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF
def create_ohe(df, col):
    le = LabelEncoder()
    a = le.fit_transform(df_new[col]).reshape(-1, 1)
    ohe = OneHotEncoder(sparse=False)
    column_names = [col + "_" + str(i) for i in le.classes_]
    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))

# Since the above function converts the column, one at a time
# We create a loop to create the final dataset with all features
temp = df_new[numeric_columns]
for column in categorical_columns:
    temp_df = create_ohe(df_new, column)
    temp = pd.concat([temp, temp_df], axis=1)

temp["StateHoliday"] = np.where(temp["StateHoliday"] == '0', 0, 1)

# Create train and test dataset with an 80:20 split
x_train, x_test, y_train, y_test = train_test_split(temp, df_new[target], test_size=0.2, random_state=2018)
# Further divide training dataset into train and validation dataset with an 90:10 split
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)



#Create Deep Neural Network Architecture
model = Sequential()
model.add(Dense(150,input_dim = 44,activation="relu"))      #The input_dim =44, since the width of the training data=44 (refer data engg section)
model.add(Dense(1,activation = "linear"))

#Configure the model
model.compile(optimizer='adam',loss="mean_absolute_error", metrics=["mean_absolute_error"])

#Train the model
model.fit(x_train.values,y_train.values, validation_data= (x_val,y_val),epochs=10,batch_size=64)
```
<details markdown="1">
<summary class='jb-small' style="color:blue">TRAINING RESULT</summary>
<hr class='division3'>
```
Train on 732390 samples, validate on 81377 samples
Epoch 1/10
732390/732390 [==============================] - 16s 22us/step - loss: 961.6725 - mean_absolute_error: 961.6725 - val_loss: 832.9768 - val_mean_absolute_error: 832.9768
Epoch 2/10
732390/732390 [==============================] - 15s 20us/step - loss: 788.6400 - mean_absolute_error: 788.6400 - val_loss: 750.9315 - val_mean_absolute_error: 750.9315
Epoch 3/10
732390/732390 [==============================] - 14s 19us/step - loss: 745.1940 - mean_absolute_error: 745.1940 - val_loss: 729.6170 - val_mean_absolute_error: 729.6170
Epoch 4/10
732390/732390 [==============================] - 13s 18us/step - loss: 728.3255 - mean_absolute_error: 728.3255 - val_loss: 714.6160 - val_mean_absolute_error: 714.6160
Epoch 5/10
732390/732390 [==============================] - 14s 19us/step - loss: 717.4465 - mean_absolute_error: 717.4465 - val_loss: 708.1066 - val_mean_absolute_error: 708.1066
Epoch 6/10
732390/732390 [==============================] - 14s 19us/step - loss: 708.3802 - mean_absolute_error: 708.3802 - val_loss: 748.1029 - val_mean_absolute_error: 748.1029
Epoch 7/10
732390/732390 [==============================] - 15s 20us/step - loss: 703.5348 - mean_absolute_error: 703.5348 - val_loss: 713.9228 - val_mean_absolute_error: 713.9228
Epoch 8/10
732390/732390 [==============================] - 14s 19us/step - loss: 698.0711 - mean_absolute_error: 698.0711 - val_loss: 694.0802 - val_mean_absolute_error: 694.0802
Epoch 9/10
732390/732390 [==============================] - 14s 19us/step - loss: 693.7195 - mean_absolute_error: 693.7195 - val_loss: 689.4929 - val_mean_absolute_error: 689.4929
Epoch 10/10
732390/732390 [==============================] - 14s 19us/step - loss: 688.7827 - mean_absolute_error: 688.7827 - val_loss: 687.1623 - val_mean_absolute_error: 687.1623
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Testing the Model Performance</summary>
<hr class='division3'>
```python
#Use the model's evaluate method to predict and evaluate the test datasets 
result = model.evaluate(x_test.values,y_test.values)

#Print the results
for i in range(len(model.metrics_names)):    
    print("Metric ",model.metrics_names[i],":",str(round(result[i],2)))
```
`OUTPUT`
```
203442/203442 [==============================] - 3s 14us/step
Metric  loss : 684.04
Metric  mean_absolute_error : 684.04
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Improving the Model 1</summary>
<hr class='division3'>
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense, Dropout



df = pd.read_csv("train.csv")
store = pd.read_csv("store.csv")
df_new = df.merge(store, on=["Store"], how="inner")

# We can extract all date properties from a datetime datatype
df_new['Date'] = pd.to_datetime(df_new['Date'], infer_datetime_format=True)
df_new["Month"] = df_new["Date"].dt.month
df_new["Quarter"] = df_new["Date"].dt.quarter
df_new["Year"] = df_new["Date"].dt.year
df_new["Day"] = df_new["Date"].dt.day
df_new["Week"] = df_new["Date"].dt.week
df_new["Season"] = np.where(df_new["Month"].isin([3, 4, 5]), "Spring",
                            np.where(df_new["Month"].isin([6, 7, 8]), "Summer",
                                     np.where(df_new["Month"].isin([9, 10, 11]), "Fall",
                                              np.where(df_new["Month"].isin([12, 1, 2]), "Winter", "None"))))

# Replace nulls with the mode
df_new["CompetitionDistance"] = df_new["CompetitionDistance"].fillna(df_new["CompetitionDistance"].mode()[0])

# Double check if we still see nulls for the column
df_new["CompetitionDistance"].isnull().sum() / df_new.shape[0] * 100

# Define a variable for each type of feature
target = ["Sales"]
numeric_columns = ["Customers", "Open", "Promo", "Promo2", "StateHoliday", "SchoolHoliday", "CompetitionDistance"]
categorical_columns = ["DayOfWeek", "Quarter", "Month", "Year", "StoreType", "Assortment", "Season"]



# Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF
def create_ohe(df, col):
    le = LabelEncoder()
    a = le.fit_transform(df_new[col]).reshape(-1, 1)
    ohe = OneHotEncoder(sparse=False)
    column_names = [col + "_" + str(i) for i in le.classes_]
    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))

# Since the above function converts the column, one at a time
# We create a loop to create the final dataset with all features
temp = df_new[numeric_columns]
for column in categorical_columns:
    temp_df = create_ohe(df_new, column)
    temp = pd.concat([temp, temp_df], axis=1)

temp["StateHoliday"] = np.where(temp["StateHoliday"] == '0', 0, 1)

# Create train and test dataset with an 80:20 split
x_train, x_test, y_train, y_test = train_test_split(temp, df_new[target], test_size=0.2, random_state=2018)
# Further divide training dataset into train and validation dataset with an 90:10 split
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)

#Create Deep Neural Network Architecture
model = Sequential()
model.add(Dense(150,input_dim = 44,activation="relu")) 
model.add(Dense(150,activation="relu"))
model.add(Dense(150,activation="relu"))
model.add(Dense(1,activation = "linear"))
model.compile(optimizer='adam',loss="mean_squared_error",metrics=["mean_absolute_error"])
model.fit(x_train,y_train, validation_data=(x_val, y_val),epochs=10,batch_size=64)

result = model.evaluate(x_test,y_test) 
for i in range(len(model.metrics_names)): 
    print("Metric ",model.metrics_names[i],":",str(round(result[i],2)))
```
`OUTPUT`
```
Train on 732390 samples, validate on 81377 samples
Epoch 1/10
732390/732390 [==============================] - 37s 50us/step - loss: 1751824.2928 - mean_absolute_error: 856.5363 - val_loss: 1177755.9705 - val_mean_absolute_error: 740.5539
Epoch 2/10
732390/732390 [==============================] - 29s 40us/step - loss: 1160876.5663 - mean_absolute_error: 723.2560 - val_loss: 1121593.8268 - val_mean_absolute_error: 706.5737
Epoch 3/10
732390/732390 [==============================] - 30s 41us/step - loss: 1087959.8210 - mean_absolute_error: 699.7806 - val_loss: 1019937.0690 - val_mean_absolute_error: 680.0693
Epoch 4/10
732390/732390 [==============================] - 33s 45us/step - loss: 1055263.2415 - mean_absolute_error: 689.1528 - val_loss: 985578.7209 - val_mean_absolute_error: 668.9547
Epoch 5/10
732390/732390 [==============================] - 31s 43us/step - loss: 1025371.6030 - mean_absolute_error: 679.8681 - val_loss: 1125251.2369 - val_mean_absolute_error: 728.9359
Epoch 6/10
732390/732390 [==============================] - 31s 42us/step - loss: 1000331.7273 - mean_absolute_error: 673.0017 - val_loss: 974033.9541 - val_mean_absolute_error: 665.9759
Epoch 7/10
732390/732390 [==============================] - 31s 42us/step - loss: 980059.8089 - mean_absolute_error: 667.9678 - val_loss: 899603.4905 - val_mean_absolute_error: 645.6330
Epoch 8/10
732390/732390 [==============================] - 31s 42us/step - loss: 959681.5355 - mean_absolute_error: 660.6007 - val_loss: 901967.1271 - val_mean_absolute_error: 644.1287
Epoch 9/10
732390/732390 [==============================] - 31s 42us/step - loss: 938922.9098 - mean_absolute_error: 653.8427 - val_loss: 921845.1646 - val_mean_absolute_error: 661.7743
Epoch 10/10
732390/732390 [==============================] - 31s 43us/step - loss: 918371.6692 - mean_absolute_error: 646.9734 - val_loss: 868885.5710 - val_mean_absolute_error: 631.0993
Metric  loss : 680.39
Metric  mean_absolute_error : 680.39
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Improving the Model 2</summary>
<hr class='division3'>
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense, Dropout



df = pd.read_csv("train.csv")
store = pd.read_csv("store.csv")
df_new = df.merge(store, on=["Store"], how="inner")

# We can extract all date properties from a datetime datatype
df_new['Date'] = pd.to_datetime(df_new['Date'], infer_datetime_format=True)
df_new["Month"] = df_new["Date"].dt.month
df_new["Quarter"] = df_new["Date"].dt.quarter
df_new["Year"] = df_new["Date"].dt.year
df_new["Day"] = df_new["Date"].dt.day
df_new["Week"] = df_new["Date"].dt.week
df_new["Season"] = np.where(df_new["Month"].isin([3, 4, 5]), "Spring",
                            np.where(df_new["Month"].isin([6, 7, 8]), "Summer",
                                     np.where(df_new["Month"].isin([9, 10, 11]), "Fall",
                                              np.where(df_new["Month"].isin([12, 1, 2]), "Winter", "None"))))

# Replace nulls with the mode
df_new["CompetitionDistance"] = df_new["CompetitionDistance"].fillna(df_new["CompetitionDistance"].mode()[0])

# Double check if we still see nulls for the column
df_new["CompetitionDistance"].isnull().sum() / df_new.shape[0] * 100

# Define a variable for each type of feature
target = ["Sales"]
numeric_columns = ["Customers", "Open", "Promo", "Promo2", "StateHoliday", "SchoolHoliday", "CompetitionDistance"]
categorical_columns = ["DayOfWeek", "Quarter", "Month", "Year", "StoreType", "Assortment", "Season"]



# Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF
def create_ohe(df, col):
    le = LabelEncoder()
    a = le.fit_transform(df_new[col]).reshape(-1, 1)
    ohe = OneHotEncoder(sparse=False)
    column_names = [col + "_" + str(i) for i in le.classes_]
    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))

# Since the above function converts the column, one at a time
# We create a loop to create the final dataset with all features
temp = df_new[numeric_columns]
for column in categorical_columns:
    temp_df = create_ohe(df_new, column)
    temp = pd.concat([temp, temp_df], axis=1)

temp["StateHoliday"] = np.where(temp["StateHoliday"] == '0', 0, 1)

# Create train and test dataset with an 80:20 split
x_train, x_test, y_train, y_test = train_test_split(temp, df_new[target], test_size=0.2, random_state=2018)
# Further divide training dataset into train and validation dataset with an 90:10 split
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)

#Create Deep Neural Network Architecture
model = Sequential()
model.add(Dense(150,input_dim = 44,activation="relu")) 
model.add(Dense(150,activation="relu")) 
model.add(Dense(150,activation="relu")) 
model.add(Dense(150,activation="relu")) 
model.add(Dense(150,activation="relu")) 
model.add(Dense(1,activation = "linear"))
model.compile(optimizer='adam',loss="mean_squared_error",metrics=["mean_absolute_error"])
model.fit(x_train,y_train, validation_data=(x_val,y_val), epochs=15,batch_size=64)

result = model.evaluate(x_test,y_test)
for i in range(len(model.metrics_names)):  
    print("Metric ",model.metrics_names[i],":",str(round(result [i],2)))
```
`OUTPUT`
```
Train on 732390 samples, validate on 81377 samples
Epoch 1/15
732390/732390 [==============================] - 47s 64us/step - loss: 1693355.7884 - mean_absolute_error: 851.3013 - val_loss: 1145939.5552 - val_mean_absolute_error: 723.8296
Epoch 2/15
732390/732390 [==============================] - 44s 60us/step - loss: 1161802.7680 - mean_absolute_error: 722.7223 - val_loss: 1047540.8904 - val_mean_absolute_error: 697.5821
Epoch 3/15
732390/732390 [==============================] - 44s 59us/step - loss: 1096405.1840 - mean_absolute_error: 701.0095 - val_loss: 1066299.1614 - val_mean_absolute_error: 686.2207
Epoch 4/15
732390/732390 [==============================] - 44s 60us/step - loss: 1059591.2408 - mean_absolute_error: 689.2741 - val_loss: 1115480.0252 - val_mean_absolute_error: 722.9864
Epoch 5/15
732390/732390 [==============================] - 44s 60us/step - loss: 1037127.7020 - mean_absolute_error: 682.4807 - val_loss: 972973.2906 - val_mean_absolute_error: 678.2566
Epoch 6/15
732390/732390 [==============================] - 44s 60us/step - loss: 1010953.3777 - mean_absolute_error: 674.9382 - val_loss: 954370.4462 - val_mean_absolute_error: 665.2246
Epoch 7/15
732390/732390 [==============================] - 44s 60us/step - loss: 998037.4484 - mean_absolute_error: 670.7878 - val_loss: 1012626.8049 - val_mean_absolute_error: 675.3255
Epoch 8/15
732390/732390 [==============================] - 45s 61us/step - loss: 974287.9476 - mean_absolute_error: 663.3675 - val_loss: 897542.6413 - val_mean_absolute_error: 648.6293
Epoch 9/15
732390/732390 [==============================] - 46s 63us/step - loss: 952037.8441 - mean_absolute_error: 656.2808 - val_loss: 936938.8103 - val_mean_absolute_error: 663.9085
Epoch 10/15
732390/732390 [==============================] - 44s 60us/step - loss: 939312.1822 - mean_absolute_error: 651.3833 - val_loss: 874905.7340 - val_mean_absolute_error: 631.0943
Epoch 11/15
732390/732390 [==============================] - 43s 58us/step - loss: 918155.9017 - mean_absolute_error: 645.2943 - val_loss: 898404.6137 - val_mean_absolute_error: 650.1163
Epoch 12/15
732390/732390 [==============================] - 43s 58us/step - loss: 905877.2954 - mean_absolute_error: 639.9159 - val_loss: 899350.7806 - val_mean_absolute_error: 635.1855
Epoch 13/15
732390/732390 [==============================] - 42s 57us/step - loss: 895967.5860 - mean_absolute_error: 636.0505 - val_loss: 905506.2252 - val_mean_absolute_error: 641.3708
Epoch 14/15
732390/732390 [==============================] - 42s 57us/step - loss: 885962.6494 - mean_absolute_error: 632.3153 - val_loss: 831220.6510 - val_mean_absolute_error: 616.1441
Epoch 15/15
732390/732390 [==============================] - 43s 58us/step - loss: 876729.0114 - mean_absolute_error: 629.1743 - val_loss: 888962.6661 - val_mean_absolute_error: 641.7786
203442/203442 [==============================] - 7s 35us/step
Metric  loss : 882949.09
Metric  mean_absolute_error : 637.69
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Improving the Model 3</summary>
<hr class='division3'>
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense, Dropout



df = pd.read_csv("train.csv")
store = pd.read_csv("store.csv")
df_new = df.merge(store, on=["Store"], how="inner")

# We can extract all date properties from a datetime datatype
df_new['Date'] = pd.to_datetime(df_new['Date'], infer_datetime_format=True)
df_new["Month"] = df_new["Date"].dt.month
df_new["Quarter"] = df_new["Date"].dt.quarter
df_new["Year"] = df_new["Date"].dt.year
df_new["Day"] = df_new["Date"].dt.day
df_new["Week"] = df_new["Date"].dt.week
df_new["Season"] = np.where(df_new["Month"].isin([3, 4, 5]), "Spring",
                            np.where(df_new["Month"].isin([6, 7, 8]), "Summer",
                                     np.where(df_new["Month"].isin([9, 10, 11]), "Fall",
                                              np.where(df_new["Month"].isin([12, 1, 2]), "Winter", "None"))))

# Replace nulls with the mode
df_new["CompetitionDistance"] = df_new["CompetitionDistance"].fillna(df_new["CompetitionDistance"].mode()[0])

# Double check if we still see nulls for the column
df_new["CompetitionDistance"].isnull().sum() / df_new.shape[0] * 100

# Define a variable for each type of feature
target = ["Sales"]
numeric_columns = ["Customers", "Open", "Promo", "Promo2", "StateHoliday", "SchoolHoliday", "CompetitionDistance"]
categorical_columns = ["DayOfWeek", "Quarter", "Month", "Year", "StoreType", "Assortment", "Season"]



# Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF
def create_ohe(df, col):
    le = LabelEncoder()
    a = le.fit_transform(df_new[col]).reshape(-1, 1)
    ohe = OneHotEncoder(sparse=False)
    column_names = [col + "_" + str(i) for i in le.classes_]
    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))

# Since the above function converts the column, one at a time
# We create a loop to create the final dataset with all features
temp = df_new[numeric_columns]
for column in categorical_columns:
    temp_df = create_ohe(df_new, column)
    temp = pd.concat([temp, temp_df], axis=1)

temp["StateHoliday"] = np.where(temp["StateHoliday"] == '0', 0, 1)

# Create train and test dataset with an 80:20 split
x_train, x_test, y_train, y_test = train_test_split(temp, df_new[target], test_size=0.2, random_state=2018)
# Further divide training dataset into train and validation dataset with an 90:10 split
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)

#Create Deep Neural Network Architecture
model = Sequential()
model.add(Dense(350,input_dim = 44,activation="relu"))
model.add(Dense(350,activation="relu")) 
model.add(Dense(1,activation = "linear"))
model.compile(optimizer='adam',loss="mean_squared_error",metrics=["mean_absolute_error"])
model.fit(x_train,y_train, validation_data=(x_val,y_val), epochs=15,batch_size=64)

result = model.evaluate(x_test,y_test) 
for i in range(len(model.metrics_names)):  
    print("Metric ",model.metrics_names[i],":", str(round(result[i],2)))
```
`OUTPUT`
```
Train on 732390 samples, validate on 81377 samples
Epoch 1/15
732390/732390 [==============================] - 50s 69us/step - loss: 1696801.1742 - mean_absolute_error: 852.3380 - val_loss: 1317084.3249 - val_mean_absolute_error: 789.5379
Epoch 2/15
732390/732390 [==============================] - 52s 71us/step - loss: 1155713.0256 - mean_absolute_error: 727.1582 - val_loss: 1062183.7259 - val_mean_absolute_error: 691.4468
Epoch 3/15
732390/732390 [==============================] - 53s 72us/step - loss: 1087896.8604 - mean_absolute_error: 704.8605 - val_loss: 1023563.1386 - val_mean_absolute_error: 688.5022
Epoch 4/15
732390/732390 [==============================] - 52s 72us/step - loss: 1058152.5636 - mean_absolute_error: 695.0822 - val_loss: 986651.8070 - val_mean_absolute_error: 670.2612
Epoch 5/15
732390/732390 [==============================] - 56s 76us/step - loss: 1032482.6717 - mean_absolute_error: 686.3516 - val_loss: 1067894.5107 - val_mean_absolute_error: 694.0162
Epoch 6/15
732390/732390 [==============================] - 52s 72us/step - loss: 1011641.7031 - mean_absolute_error: 679.8068 - val_loss: 969257.7984 - val_mean_absolute_error: 676.1887
Epoch 7/15
732390/732390 [==============================] - 53s 72us/step - loss: 992370.3198 - mean_absolute_error: 673.6577 - val_loss: 1004949.4962 - val_mean_absolute_error: 700.5331
Epoch 8/15
732390/732390 [==============================] - 53s 73us/step - loss: 974573.7028 - mean_absolute_error: 667.2528 - val_loss: 937751.6322 - val_mean_absolute_error: 651.9460
Epoch 9/15
732390/732390 [==============================] - 54s 74us/step - loss: 958369.8043 - mean_absolute_error: 661.7424 - val_loss: 990676.9595 - val_mean_absolute_error: 684.4022
Epoch 10/15
732390/732390 [==============================] - 54s 74us/step - loss: 943994.1679 - mean_absolute_error: 657.4421 - val_loss: 917064.5527 - val_mean_absolute_error: 658.8234
Epoch 11/15
732390/732390 [==============================] - 54s 74us/step - loss: 931808.3522 - mean_absolute_error: 652.8717 - val_loss: 941671.1141 - val_mean_absolute_error: 667.7861
Epoch 12/15
732390/732390 [==============================] - 54s 74us/step - loss: 918254.2089 - mean_absolute_error: 647.1999 - val_loss: 1010805.0772 - val_mean_absolute_error: 681.9616
Epoch 13/15
732390/732390 [==============================] - 52s 71us/step - loss: 908330.2797 - mean_absolute_error: 643.4899 - val_loss: 898364.6416 - val_mean_absolute_error: 644.6445
Epoch 14/15
732390/732390 [==============================] - 57s 78us/step - loss: 894027.9716 - mean_absolute_error: 638.6149 - val_loss: 860392.6973 - val_mean_absolute_error: 633.0040
Epoch 15/15
732390/732390 [==============================] - 58s 79us/step - loss: 884101.0426 - mean_absolute_error: 634.8977 - val_loss: 850341.6795 - val_mean_absolute_error: 627.0114
203442/203442 [==============================] - 9s 44us/step
Metric  loss : 854548.0
Metric  mean_absolute_error : 624.18
```
<hr class='division3'>
</details>
<details markdown="1">
<summary class='jb-small' style="color:blue">Plotting the Loss Metric Across Epochs</summary>
<hr class='division3'>
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import History
import matplotlib.pyplot as plt
%matplotlib inline


df = pd.read_csv("train.csv")
store = pd.read_csv("store.csv")
df_new = df.merge(store, on=["Store"], how="inner")

# We can extract all date properties from a datetime datatype
df_new['Date'] = pd.to_datetime(df_new['Date'], infer_datetime_format=True)
df_new["Month"] = df_new["Date"].dt.month
df_new["Quarter"] = df_new["Date"].dt.quarter
df_new["Year"] = df_new["Date"].dt.year
df_new["Day"] = df_new["Date"].dt.day
df_new["Week"] = df_new["Date"].dt.week
df_new["Season"] = np.where(df_new["Month"].isin([3, 4, 5]), "Spring",
                            np.where(df_new["Month"].isin([6, 7, 8]), "Summer",
                                     np.where(df_new["Month"].isin([9, 10, 11]), "Fall",
                                              np.where(df_new["Month"].isin([12, 1, 2]), "Winter", "None"))))

# Replace nulls with the mode
df_new["CompetitionDistance"] = df_new["CompetitionDistance"].fillna(df_new["CompetitionDistance"].mode()[0])

# Double check if we still see nulls for the column
df_new["CompetitionDistance"].isnull().sum() / df_new.shape[0] * 100

# Define a variable for each type of feature
target = ["Sales"]
numeric_columns = ["Customers", "Open", "Promo", "Promo2", "StateHoliday", "SchoolHoliday", "CompetitionDistance"]
categorical_columns = ["DayOfWeek", "Quarter", "Month", "Year", "StoreType", "Assortment", "Season"]



# Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF
def create_ohe(df, col):
    le = LabelEncoder()
    a = le.fit_transform(df_new[col]).reshape(-1, 1)
    ohe = OneHotEncoder(sparse=False)
    column_names = [col + "_" + str(i) for i in le.classes_]
    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))

# Since the above function converts the column, one at a time
# We create a loop to create the final dataset with all features
temp = df_new[numeric_columns]
for column in categorical_columns:
    temp_df = create_ohe(df_new, column)
    temp = pd.concat([temp, temp_df], axis=1)

temp["StateHoliday"] = np.where(temp["StateHoliday"] == '0', 0, 1)

# Create train and test dataset with an 80:20 split
x_train, x_test, y_train, y_test = train_test_split(temp, df_new[target], test_size=0.2, random_state=2018)
# Further divide training dataset into train and validation dataset with an 90:10 split
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)

#Create Deep Neural Network Architecture
history = History()
model = Sequential() 
model.add(Dense(350,input_dim = 44,activation="relu"))
model.add(Dense(350,activation="relu")) 
model.add(Dense(350,activation="relu"))
model.add(Dense(350,activation="relu"))
model.add(Dense(350,activation="relu")) 
model.add(Dense(1,activation = "linear"))

model.compile(optimizer='adam',loss="mean_squared_error",metrics=["mean_absolute_error"])
model.fit(x_train,y_train, validation_data=(x_val,y_val), epochs=15,batch_size=64,callbacks=[history])

result = model.evaluate(x_test,y_test)
for i in range(len(model.metrics_names)):  
    print("Metric ",model.metrics_names[i],":",str(round(result [i],2)))
    
    
# visualization for metric
plt.plot(history.history['loss']) 
plt.plot(history.history['val_loss'])
plt.title("Model's Training & Validation loss across epochs")
plt.ylabel('Loss') 
plt.xlabel('Epochs') 
plt.legend(['Train', 'Validation'], loc='upper right') 
plt.show()
```
`OUTPUT`
```
Train on 732390 samples, validate on 81377 samples
Epoch 1/15
732390/732390 [==============================] - 142s 195us/step - loss: 1633522.8718 - mean_absolute_error: 838.6379 - val_loss: 1217993.0883 - val_mean_absolute_error: 732.2864
Epoch 2/15
732390/732390 [==============================] - 146s 200us/step - loss: 1179818.2841 - mean_absolute_error: 726.9929 - val_loss: 1088124.4735 - val_mean_absolute_error: 710.5632
Epoch 3/15
732390/732390 [==============================] - 145s 198us/step - loss: 1110881.7459 - mean_absolute_error: 705.6384 - val_loss: 1019603.4330 - val_mean_absolute_error: 679.5359
Epoch 4/15
732390/732390 [==============================] - 148s 202us/step - loss: 1073248.9016 - mean_absolute_error: 693.0075 - val_loss: 1034116.4268 - val_mean_absolute_error: 677.2357
Epoch 5/15
732390/732390 [==============================] - 144s 197us/step - loss: 1042131.6708 - mean_absolute_error: 683.3028 - val_loss: 971867.9645 - val_mean_absolute_error: 674.6207
Epoch 6/15
732390/732390 [==============================] - 134s 182us/step - loss: 1022145.1901 - mean_absolute_error: 677.9384 - val_loss: 1123250.9915 - val_mean_absolute_error: 730.9583
Epoch 7/15
732390/732390 [==============================] - 131s 178us/step - loss: 998527.2257 - mean_absolute_error: 671.0589 - val_loss: 950288.8741 - val_mean_absolute_error: 666.2436
Epoch 8/15
732390/732390 [==============================] - 133s 182us/step - loss: 979936.5206 - mean_absolute_error: 665.6808 - val_loss: 913612.6051 - val_mean_absolute_error: 647.3661
Epoch 9/15
732390/732390 [==============================] - 136s 186us/step - loss: 959011.7640 - mean_absolute_error: 658.8191 - val_loss: 877278.1993 - val_mean_absolute_error: 633.0947
Epoch 10/15
732390/732390 [==============================] - 138s 189us/step - loss: 944423.2573 - mean_absolute_error: 653.6127 - val_loss: 940223.5264 - val_mean_absolute_error: 650.9949
Epoch 11/15
732390/732390 [==============================] - 134s 183us/step - loss: 921521.4840 - mean_absolute_error: 646.2910 - val_loss: 922973.2254 - val_mean_absolute_error: 659.6578
Epoch 12/15
732390/732390 [==============================] - 136s 185us/step - loss: 908583.9544 - mean_absolute_error: 641.5706 - val_loss: 846290.7568 - val_mean_absolute_error: 625.4439
Epoch 13/15
732390/732390 [==============================] - 133s 182us/step - loss: 896627.8529 - mean_absolute_error: 637.2651 - val_loss: 830765.3276 - val_mean_absolute_error: 619.4513
Epoch 14/15
732390/732390 [==============================] - 15480s 21ms/step - loss: 881627.1912 - mean_absolute_error: 632.3052 - val_loss: 878429.4993 - val_mean_absolute_error: 630.2251
Epoch 15/15
732390/732390 [==============================] - 178s 243us/step - loss: 868846.1121 - mean_absolute_error: 627.2891 - val_loss: 859102.4735 - val_mean_absolute_error: 631.2212
203442/203442 [==============================] - 14s 69us/step
Metric  loss : 855410.89
Metric  mean_absolute_error : 626.6
```
![다운로드](https://user-images.githubusercontent.com/52376448/65430400-d4c6e580-de52-11e9-8b50-f04e47e08b62.png)
<hr class='division3'>
</details>
<br><br><br>


<hr class="division2">

## **Deep Neural Networks for Supervised Learning: Classification**
<a href="https://www.kaggle.com/c/predicting-red-hat-business-value" target="_blank">Dataset Download</a>
<br><br><br>


### ***Exploring the Data***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Data Engineering***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Defining Model Baseline Accuracy***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Designing the DNN for Classification***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Revisiting the Data***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***DNNs for Classification with Improved Data***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>


<hr class="division2">

## **Tuning and Deploying Deep Neural Networks**

### ***What Is Regularization***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---

### ***Hyperparameter Tuning***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

---


### ***Model Deployment***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>


<hr class="division2">

## **The Path Ahead**

### ***What’s Next for DL Expertise***

```python
```
<details markdown="1">
<summary class='jb-small' style="color:blue">OUTPUT</summary>
<hr class='division3'>
<hr class='division3'>
</details>
<br><br><br>

<hr class="division1">

List of posts followed by this article
- [post1](https://userdyk-github.github.io/)
- <a href='https://userdyk-github.github.io/'>post2</a>
- <a href='https://userdyk-github.github.io/'>post3</a>

---

Reference
- Jojo Moolayil, Learn Keras for Deep Neural Networks, 2019
- <a href='https://userdyk-github.github.io/'>post2</a>
- <a href='https://userdyk-github.github.io/'>post3</a>

---





